{"cells":[{"cell_type":"markdown","metadata":{"id":"b8MO4w7-BklO"},"source":["# Importing packages and datasets."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":257,"status":"ok","timestamp":1694601073998,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"UznGqZzuBjJC"},"outputs":[],"source":["import pandas\n","import numpy\n","\n","import altair\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.neural_network import MLPRegressor\n","\n","from sklearn.metrics import mean_squared_error\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17434,"status":"ok","timestamp":1694600452824,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"UAsuDAQVBsit","outputId":"5e453ce7-fb52-4d56-c4db-c714e7cbbd79"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","base_file_path = \"/content/drive/MyDrive/\"\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["base_file_path = \"../data/processed/\"\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1694600453886,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"h2byloZCBuiY","outputId":"3bc03578-78b1-4608-919f-6bb393cf0714"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2895, 30)\n"]}],"source":["dataFrame = pandas.read_csv(base_file_path + \"cleaned.csv\")\n","print(dataFrame.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1694601874932,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"8FvRPv4BGurv","outputId":"fc18e787-b8e4-4d99-8349-bed7908b6d95"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2895, 30)\n"]}],"source":["normalisedDataFrame = pandas.read_csv(base_file_path + \"normalised.csv\")\n","print(normalisedDataFrame.shape)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1694601617113,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"B6Vu9IFgDxGt","outputId":"8b28a6e3-de11-49f9-8042-fc85d7e9b55a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['avgAnnCount', 'avgDeathsPerYear', 'TARGET_deathRate', 'incidenceRate',\n","       'medIncome', 'popEst2015', 'povertyPercent', 'studyPerCap', 'MedianAge',\n","       'MedianAgeMale', 'MedianAgeFemale', 'AvgHouseholdSize',\n","       'PercentMarried', 'PctNoHS18_24', 'PctHS18_24', 'PctBachDeg18_24',\n","       'PctHS25_Over', 'PctBachDeg25_Over', 'PctEmployed16_Over',\n","       'PctUnemployed16_Over', 'PctPrivateCoverage', 'PctEmpPrivCoverage',\n","       'PctPublicCoverage', 'PctPublicCoverageAlone', 'PctWhite', 'PctBlack',\n","       'PctAsian', 'PctOtherRace', 'PctMarriedHouseholds', 'BirthRate'],\n","      dtype='object')\n"]}],"source":["print(dataFrame.columns)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1694600577542,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"BIjAnODgB77t"},"outputs":[],"source":["possibly_related_fields = [\n","                           \"medIncome\", \"povertyPercent\", \"PctUnemployed16_Over\", \"PctEmployed16_Over\",\n","                           \"PctHS25_Over\", \"PctBachDeg25_Over\",\n","                           \"PctPrivateCoverage\", \"PctEmpPrivCoverage\", \"PctPublicCoverage\", \"PctPublicCoverageAlone\",\n","                           \"PctMarriedHouseholds\", \"PercentMarried\"\n","                           ]\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1694601628294,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"z78m9A9bBwmo","outputId":"e81384ae-5c64-4212-bb05-d4211cfba840"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2395, 12) \t (500, 12)\n","(2395,) \t (500,)\n"]}],"source":["X = dataFrame[possibly_related_fields + [\"TARGET_deathRate\"]].copy()\n","y = X.pop(\"TARGET_deathRate\")\n","\n","train, test = train_test_split(dataFrame, test_size=0.1727, random_state=42)\n","df_train, df_test = train_test_split(X, test_size=0.1727, random_state=42)\n","y_train, y_test = train_test_split(y, test_size=0.1727, random_state=42)\n","\n","print(df_train.shape, \"\\t\", df_test.shape)\n","print(y_train.shape, \"\\t\", y_test.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1694601919928,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"Bq7O-LQpHS2U","outputId":"ccebb510-ad71-42d0-d4b4-cbc235267d41"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2395, 12) \t (500, 12)\n","(2395,) \t (500,)\n"]}],"source":["X = normalisedDataFrame[possibly_related_fields + [\"TARGET_deathRate\"]].copy()\n","y = X.pop(\"TARGET_deathRate\")\n","\n","train, test = train_test_split(dataFrame, test_size=0.1727, random_state=42)\n","df_train, df_test = train_test_split(X, test_size=0.1727, random_state=42)\n","y_train, y_test = train_test_split(y, test_size=0.1727, random_state=42)\n","\n","print(df_train.shape, \"\\t\", df_test.shape)\n","print(y_train.shape, \"\\t\", y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"SJvCrXorB-YN"},"source":["# Baseline Performance."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1694601923174,"user":{"displayName":"Tarun Krishnan","userId":"04307794948231036457"},"user_tz":-600},"id":"nNIngxn-B9rZ","outputId":"b03a9d0a-97b5-49b0-f9c1-ab23644c78ca"},"outputs":[{"data":{"text/plain":["712.9935696334397"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["y_mean = y.mean()\n","\n","y_base = numpy.full((y_test.shape[0], ), y_mean)\n","(y_base - y_test).sum()\n","\n","mean_squared_error(y_test, y_base, squared=True)\n"]},{"cell_type":"markdown","metadata":{"id":"rchu0lWxEWIt"},"source":["# Training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nnRegressor = neuralNetwork = MLPRegressor(\n","\t\thidden_layer_sizes=[128], \t\t\t\t#array-like of shape(n_layers - 2,), default=(100,)\n","\t\tbatch_size=\"auto\", \t\t\t\t\t\t#int, default=’auto’\n","\t\tactivation=\"relu\", \t\t\t\t\t\t#{’identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n","\t\tsolver=\"adam\", \t\t\t\t\t\t\t#{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n","\t\talpha=0.0001,\t\t\t\t\t\t\t#float, default=0.0001\n","\t\tlearning_rate =\"constant\", \t\t\t\t#{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’\n","\t\tlearning_rate_init = 0.001,\t\t\t\t#float, default=0.001\n","\t\tpower_t=0.5,\t\t\t\t\t\t\t#float, default=0.5\n","\t\tmax_iter=200,\t\t\t\t\t\t\t#int, default=200\n","\t\tshuffle=False,\t\t\t\t\t\t\t#bool, default=True, used when solver=’sgd’ or ‘adam’\n","\t\trandom_state=1,\n","\t\ttol=1e-4,\t\t\t\t\t\t\t\t#float, default=1e-4\n","\t\tverbose=True\t\t\t\t\t\t\t#bool, default=False\n",")\n","\n","regression = nnRegressor.fit(df_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Testing."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def results(test_type):\n","\n","\tmse = 0.0\n","\tprediction_line = altair.Chart()\n","\tactual_line = altair.Chart()\n","\n","\tif test_type == \"test\":\n","\t\ty_preds = neuralNetwork.predict(df_test)\n","\t\tprediction_line = altair.Chart(pandas.DataFrame({'x': y_test, 'y': y_preds})).mark_line().encode(\n","      \t\tx='x',\n","      \t\ty='y',\n","\t\t\tcolor='blue'\n","    \t)\n","\t\tactual_line = altair.Chart(pandas.DataFrame({'x': y_test, 'y': y_test})).mark_line().encode(\n","      \t\tx='x',\n","      \t\ty='y',\n","\t\t\tcolor='orange'\n","    \t)\n","\t\tmse = mean_squared_error(y_test, y_preds, squared=True)\n","\n","\telif test_type == \"train\":\n","\t\ty_preds = neuralNetwork.predict(df_train)\n","\t\tprediction_line = altair.Chart(pandas.DataFrame({'x': y_train, 'y': y_preds})).mark_line().encode(\n","      \t\tx='x',\n","      \t\ty='y',\n","\t\t\tcolor='blue'\n","    \t)\n","\t\tactual_line = altair.Chart(pandas.DataFrame({'x': y_train, 'y': y_train})).mark_line().encode(\n","      \t\tx='x',\n","      \t\ty='y',\n","\t\t\t  color='orange'\n","    \t)\n","\t\tmse = mean_squared_error(y_train, y_preds, squared=True)\n","\n","\treturn (actual_line, prediction_line, mse)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 76213953.43813549\n","Iteration 2, loss = 12893415.53583313\n","Iteration 3, loss = 390520.27282529\n","Iteration 4, loss = 1014917.54955776\n","Iteration 5, loss = 366828.48774938\n","Iteration 6, loss = 17100.99825522\n","Iteration 7, loss = 36230.93849631\n","Iteration 8, loss = 8297.33450427\n"]},{"name":"stdout","output_type":"stream","text":["Iteration 9, loss = 4794.58934986\n","Iteration 10, loss = 4097.74241130\n","Iteration 11, loss = 3425.29102794\n","Iteration 12, loss = 3440.55624714\n","Iteration 13, loss = 3370.09591180\n","Iteration 14, loss = 3365.26893696\n","Iteration 15, loss = 3355.41166890\n","Iteration 16, loss = 3351.76027002\n","Iteration 17, loss = 3346.58142407\n","Iteration 18, loss = 3341.48817088\n","Iteration 19, loss = 3336.47008671\n","Iteration 20, loss = 3331.37557056\n","Iteration 21, loss = 3326.05323735\n","Iteration 22, loss = 3320.59708909\n","Iteration 23, loss = 3315.03093695\n","Iteration 24, loss = 3309.32532208\n","Iteration 25, loss = 3303.48372981\n","Iteration 26, loss = 3297.51813610\n","Iteration 27, loss = 3291.42784148\n","Iteration 28, loss = 3285.21315221\n","Iteration 29, loss = 3279.01892587\n","Iteration 30, loss = 3271.94054003\n","Iteration 31, loss = 3265.17756674\n","Iteration 32, loss = 3259.02993625\n","Iteration 33, loss = 3252.46852214\n","Iteration 34, loss = 3245.46874045\n","Iteration 35, loss = 3238.40948405\n","Iteration 36, loss = 3231.49013027\n","Iteration 37, loss = 3224.52409333\n","Iteration 38, loss = 3216.88977339\n","Iteration 39, loss = 3202.13323688\n","Iteration 40, loss = 3171.12289011\n","Iteration 41, loss = 3119.29954605\n","Iteration 42, loss = 3038.02756062\n","Iteration 43, loss = 2944.54960116\n","Iteration 44, loss = 2853.51374759\n","Iteration 45, loss = 2714.36864962\n","Iteration 46, loss = 2578.07731272\n","Iteration 47, loss = 2449.15284364\n","Iteration 48, loss = 2314.08960950\n","Iteration 49, loss = 2183.36616662\n","Iteration 50, loss = 2052.59320579\n","Iteration 51, loss = 1920.55373785\n","Iteration 52, loss = 1783.83244155\n","Iteration 53, loss = 1646.99489996\n","Iteration 54, loss = 1518.68674112\n","Iteration 55, loss = 1405.85958120\n","Iteration 56, loss = 1298.77936582\n","Iteration 57, loss = 1196.62842732\n","Iteration 58, loss = 1100.18117770\n","Iteration 59, loss = 1009.68970735\n","Iteration 60, loss = 925.42226294\n","Iteration 61, loss = 846.34286068\n","Iteration 62, loss = 771.73423630\n","Iteration 63, loss = 702.20068618\n","Iteration 64, loss = 640.37794044\n","Iteration 65, loss = 586.25956449\n","Iteration 66, loss = 539.18583728\n","Iteration 67, loss = 504.78024762\n","Iteration 68, loss = 466.97271366\n","Iteration 69, loss = 438.32255545\n","Iteration 70, loss = 417.49353914\n","Iteration 71, loss = 398.59518498\n","Iteration 72, loss = 383.70995423\n","Iteration 73, loss = 372.37809642\n","Iteration 74, loss = 362.76255159\n","Iteration 75, loss = 354.27479008\n","Iteration 76, loss = 347.44259777\n","Iteration 77, loss = 342.32846086\n","Iteration 78, loss = 338.27383097\n","Iteration 79, loss = 334.79533765\n","Iteration 80, loss = 331.87405138\n","Iteration 81, loss = 329.51148563\n","Iteration 82, loss = 327.53830641\n","Iteration 83, loss = 325.81548884\n","Iteration 84, loss = 324.71416963\n","Iteration 85, loss = 327.89822719\n","Iteration 86, loss = 345.51126414\n","Iteration 87, loss = 354.50026908\n","Iteration 88, loss = 319.30724494\n","Iteration 89, loss = 315.71940633\n","Iteration 90, loss = 320.48587614\n","Iteration 91, loss = 314.89648867\n","Iteration 92, loss = 316.40442491\n","Iteration 93, loss = 314.21372929\n","Iteration 94, loss = 313.36181296\n","Iteration 95, loss = 312.55299221\n","Iteration 96, loss = 320.56426448\n","Iteration 97, loss = 311.10613524\n","Iteration 98, loss = 318.95906935\n","Iteration 99, loss = 320.84512926\n","Iteration 100, loss = 307.28524104\n","Iteration 101, loss = 310.20167300\n","Iteration 102, loss = 309.07518159\n","Iteration 103, loss = 306.97501413\n","Iteration 104, loss = 319.45258046\n","Iteration 105, loss = 316.94187850\n","Iteration 106, loss = 302.98648123\n","Iteration 107, loss = 308.06824978\n","Iteration 108, loss = 304.92942305\n","Iteration 109, loss = 304.99309319\n","Iteration 110, loss = 304.57530657\n","Iteration 111, loss = 303.83710233\n","Iteration 112, loss = 303.58944222\n","Iteration 113, loss = 302.98435380\n","Iteration 114, loss = 302.60478672\n","Iteration 115, loss = 302.12527024\n","Iteration 116, loss = 301.69291484\n","Iteration 117, loss = 301.25844293\n","Iteration 118, loss = 300.82647012\n","Iteration 119, loss = 300.40656227\n","Iteration 120, loss = 299.98791042\n","Iteration 121, loss = 299.57787529\n","Iteration 122, loss = 299.17242012\n","Iteration 123, loss = 298.77306025\n","Iteration 124, loss = 298.37940371\n","Iteration 125, loss = 297.99123569\n","Iteration 126, loss = 297.60873644\n","Iteration 127, loss = 297.23167132\n","Iteration 128, loss = 296.98504287\n","Iteration 129, loss = 296.55670640\n","Iteration 130, loss = 296.19823144\n","Iteration 131, loss = 295.73050846\n","Iteration 132, loss = 295.54722433\n","Iteration 133, loss = 295.14937827\n","Iteration 134, loss = 294.66719783\n","Iteration 135, loss = 294.53762145\n","Iteration 136, loss = 294.13490901\n","Iteration 137, loss = 293.67166160\n","Iteration 138, loss = 296.08744287\n","Iteration 139, loss = 292.67223526\n","Iteration 140, loss = 306.68806527\n","Iteration 141, loss = 324.91128382\n","Iteration 142, loss = 302.23043226\n","Iteration 143, loss = 287.92838859\n","Iteration 144, loss = 294.63620196\n","Iteration 145, loss = 292.10950721\n","Iteration 146, loss = 290.66056327\n","Iteration 147, loss = 291.54121787\n","Iteration 148, loss = 290.65933421\n","Iteration 149, loss = 290.37583261\n","Iteration 150, loss = 290.21487265\n","Iteration 151, loss = 289.81425732\n","Iteration 152, loss = 289.56212201\n","Iteration 153, loss = 289.28994365\n","Iteration 154, loss = 289.00165009\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","609.0425319875068\n"]}],"source":["actual_line, prediction_line, mse = results(\"train\")\n","print(mse)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 76213953.43813549\n","Iteration 2, loss = 12893415.53583313\n","Iteration 3, loss = 390520.27282529\n","Iteration 4, loss = 1014917.54955776\n","Iteration 5, loss = 366828.48774938\n","Iteration 6, loss = 17100.99825522\n","Iteration 7, loss = 36230.93849631\n","Iteration 8, loss = 8297.33450427\n","Iteration 9, loss = 4794.58934986\n"]},{"name":"stdout","output_type":"stream","text":["Iteration 10, loss = 4097.74241130\n","Iteration 11, loss = 3425.29102794\n","Iteration 12, loss = 3440.55624714\n","Iteration 13, loss = 3370.09591180\n","Iteration 14, loss = 3365.26893696\n","Iteration 15, loss = 3355.41166890\n","Iteration 16, loss = 3351.76027002\n","Iteration 17, loss = 3346.58142407\n","Iteration 18, loss = 3341.48817088\n","Iteration 19, loss = 3336.47008671\n","Iteration 20, loss = 3331.37557056\n","Iteration 21, loss = 3326.05323735\n","Iteration 22, loss = 3320.59708909\n","Iteration 23, loss = 3315.03093695\n","Iteration 24, loss = 3309.32532208\n","Iteration 25, loss = 3303.48372981\n","Iteration 26, loss = 3297.51813610\n","Iteration 27, loss = 3291.42784148\n","Iteration 28, loss = 3285.21315221\n","Iteration 29, loss = 3279.01892587\n","Iteration 30, loss = 3271.94054003\n","Iteration 31, loss = 3265.17756674\n","Iteration 32, loss = 3259.02993625\n","Iteration 33, loss = 3252.46852214\n","Iteration 34, loss = 3245.46874045\n","Iteration 35, loss = 3238.40948405\n","Iteration 36, loss = 3231.49013027\n","Iteration 37, loss = 3224.52409333\n","Iteration 38, loss = 3216.88977339\n","Iteration 39, loss = 3202.13323688\n","Iteration 40, loss = 3171.12289011\n","Iteration 41, loss = 3119.29954605\n","Iteration 42, loss = 3038.02756062\n","Iteration 43, loss = 2944.54960116\n","Iteration 44, loss = 2853.51374759\n","Iteration 45, loss = 2714.36864962\n","Iteration 46, loss = 2578.07731272\n","Iteration 47, loss = 2449.15284364\n","Iteration 48, loss = 2314.08960950\n","Iteration 49, loss = 2183.36616662\n","Iteration 50, loss = 2052.59320579\n","Iteration 51, loss = 1920.55373785\n","Iteration 52, loss = 1783.83244155\n","Iteration 53, loss = 1646.99489996\n","Iteration 54, loss = 1518.68674112\n","Iteration 55, loss = 1405.85958120\n","Iteration 56, loss = 1298.77936582\n","Iteration 57, loss = 1196.62842732\n","Iteration 58, loss = 1100.18117770\n","Iteration 59, loss = 1009.68970735\n","Iteration 60, loss = 925.42226294\n","Iteration 61, loss = 846.34286068\n","Iteration 62, loss = 771.73423630\n","Iteration 63, loss = 702.20068618\n","Iteration 64, loss = 640.37794044\n","Iteration 65, loss = 586.25956449\n","Iteration 66, loss = 539.18583728\n","Iteration 67, loss = 504.78024762\n","Iteration 68, loss = 466.97271366\n","Iteration 69, loss = 438.32255545\n","Iteration 70, loss = 417.49353914\n","Iteration 71, loss = 398.59518498\n","Iteration 72, loss = 383.70995423\n","Iteration 73, loss = 372.37809642\n","Iteration 74, loss = 362.76255159\n","Iteration 75, loss = 354.27479008\n","Iteration 76, loss = 347.44259777\n","Iteration 77, loss = 342.32846086\n","Iteration 78, loss = 338.27383097\n","Iteration 79, loss = 334.79533765\n","Iteration 80, loss = 331.87405138\n","Iteration 81, loss = 329.51148563\n","Iteration 82, loss = 327.53830641\n","Iteration 83, loss = 325.81548884\n","Iteration 84, loss = 324.71416963\n","Iteration 85, loss = 327.89822719\n","Iteration 86, loss = 345.51126414\n","Iteration 87, loss = 354.50026908\n","Iteration 88, loss = 319.30724494\n","Iteration 89, loss = 315.71940633\n","Iteration 90, loss = 320.48587614\n","Iteration 91, loss = 314.89648867\n","Iteration 92, loss = 316.40442491\n","Iteration 93, loss = 314.21372929\n","Iteration 94, loss = 313.36181296\n","Iteration 95, loss = 312.55299221\n","Iteration 96, loss = 320.56426448\n","Iteration 97, loss = 311.10613524\n","Iteration 98, loss = 318.95906935\n","Iteration 99, loss = 320.84512926\n","Iteration 100, loss = 307.28524104\n","Iteration 101, loss = 310.20167300\n","Iteration 102, loss = 309.07518159\n","Iteration 103, loss = 306.97501413\n","Iteration 104, loss = 319.45258046\n","Iteration 105, loss = 316.94187850\n","Iteration 106, loss = 302.98648123\n","Iteration 107, loss = 308.06824978\n","Iteration 108, loss = 304.92942305\n","Iteration 109, loss = 304.99309319\n","Iteration 110, loss = 304.57530657\n","Iteration 111, loss = 303.83710233\n","Iteration 112, loss = 303.58944222\n","Iteration 113, loss = 302.98435380\n","Iteration 114, loss = 302.60478672\n","Iteration 115, loss = 302.12527024\n","Iteration 116, loss = 301.69291484\n","Iteration 117, loss = 301.25844293\n","Iteration 118, loss = 300.82647012\n","Iteration 119, loss = 300.40656227\n","Iteration 120, loss = 299.98791042\n","Iteration 121, loss = 299.57787529\n","Iteration 122, loss = 299.17242012\n","Iteration 123, loss = 298.77306025\n","Iteration 124, loss = 298.37940371\n","Iteration 125, loss = 297.99123569\n","Iteration 126, loss = 297.60873644\n","Iteration 127, loss = 297.23167132\n","Iteration 128, loss = 296.98504287\n","Iteration 129, loss = 296.55670640\n","Iteration 130, loss = 296.19823144\n","Iteration 131, loss = 295.73050846\n","Iteration 132, loss = 295.54722433\n","Iteration 133, loss = 295.14937827\n","Iteration 134, loss = 294.66719783\n","Iteration 135, loss = 294.53762145\n","Iteration 136, loss = 294.13490901\n","Iteration 137, loss = 293.67166160\n","Iteration 138, loss = 296.08744287\n","Iteration 139, loss = 292.67223526\n","Iteration 140, loss = 306.68806527\n","Iteration 141, loss = 324.91128382\n","Iteration 142, loss = 302.23043226\n","Iteration 143, loss = 287.92838859\n","Iteration 144, loss = 294.63620196\n","Iteration 145, loss = 292.10950721\n","Iteration 146, loss = 290.66056327\n","Iteration 147, loss = 291.54121787\n","Iteration 148, loss = 290.65933421\n","Iteration 149, loss = 290.37583261\n","Iteration 150, loss = 290.21487265\n","Iteration 151, loss = 289.81425732\n","Iteration 152, loss = 289.56212201\n","Iteration 153, loss = 289.28994365\n","Iteration 154, loss = 289.00165009\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","533.9324553000473\n"]}],"source":["actual_line, prediction_line, mse = results(\"test\")\n","print(mse)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNl7ME/mDG8L1oEwUvLpuIk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
